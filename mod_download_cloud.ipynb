{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Download_tobacco.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Kns7eDRLx-D7",
        "qSvL7SpZLvj-",
        "8IPYhT1VMS8_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71pqkH1owdoE",
        "colab_type": "text"
      },
      "source": [
        "##1. Download Script\n",
        "This script is based on the original download script found at https://github.com/wirriamm/CoNeGC/blob/master/1_Download_scripts_30Jan.ipynb, obtained on 30 January 2020.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd1c1s8M6Cy0",
        "colab_type": "text"
      },
      "source": [
        "# Getting started\n",
        "\n",
        "\n",
        "\n",
        "1.   Create folder that you want to work in on Google Drive\n",
        "2.   Download CDS file of the organism that you are working with into the folder\n",
        "3.   Create a file containint the list of SRA runIDs that you want to download\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH1vtXATwNSs",
        "colab_type": "code",
        "outputId": "7782f5fd-9f28-4f1c-c3c3-b687615ef0e8",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!rm -rf /content/sample_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kns7eDRLx-D7",
        "colab_type": "text"
      },
      "source": [
        "####Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Rvuo_MK6Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import modules\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime as dt\n",
        "import scipy.stats as stats\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeEw40gnLj66",
        "colab_type": "text"
      },
      "source": [
        "####User input of variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHNkZjz85SkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Input form {display-mode: \"form\"}\n",
        "\n",
        "#@markdown Enter the species name in the format \"_**Genus_species**_\", Eg: '_**Elaeis_guineensis**_'\n",
        "\n",
        "#@markdown The folder name in your Google Drive main directory should also go by this name.\n",
        "\n",
        "species_name = 'Ntab' #@param {type: 'string'}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown File name for Run Table (with extension), stored in Google Drive folder.\n",
        "\n",
        "#@markdown Eg: '_**RunTable_Elaeis_guineensis.txt**_'\n",
        "\n",
        "RunTable_file = 'runid_test.txt' #@param {type: 'string'}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown File name for CDS file (with extension), stored in Google Drive folder.\n",
        "\n",
        "#@markdown Eg: '_**cds.selected_transcript.egu.fasta.gz**_'\n",
        "\n",
        "cds_fasta_file = 'GCF_000715135.1_Ntab-TN90_cds_from_genomic.fna.gz' #@param {type: 'string'}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown Specify download mode\n",
        "\n",
        "download_mode = \"A. Start fresh run\" #@param [\"A. Start fresh run\", \"B. Continue from previous run\"]\n",
        "download_mode = download_mode[0]\n",
        "\n",
        "if download_mode == \"B\":\n",
        "  Date_initiated = '2020-01-30' #@param {type: 'date'}\n",
        "  date = Date_initiated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSvL7SpZLvj-",
        "colab_type": "text"
      },
      "source": [
        "####Install Dependencies, Initialise Variables and Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKhgpeyZLxJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Dependencies directory\n",
        "if os.path.exists('/content/Dependencies') == False:\n",
        "  os.mkdir('/content/Dependencies')\n",
        "  os.chdir('/content/Dependencies')\n",
        "  print('Dependencies directory created.')\n",
        "  #Download and install kallisto\n",
        "  os.system('wget \\'https://github.com/pachterlab/kallisto/releases/download/v0.46.0/kallisto_linux-v0.46.0.tar.gz\\'')\n",
        "  os.system('tar -xf kallisto_linux-v0.46.0.tar.gz')\n",
        "  if os.path.exists('kallisto/kallisto'):\n",
        "    print('kallisto installed.')\n",
        "    !cp kallisto/kallisto /bin/kallisto\n",
        "  else:\n",
        "    print('kallisto not found.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_bXo3YjLz3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define paths\n",
        "working_dir_path = \"/content/gdrive/My Drive/Projects/William_2019/\" + species_name + \"/\"\n",
        "working_dir_path_ter = \"/content/gdrive/My\\ Drive/Projects/William_2019/\" + species_name + \"/\"\n",
        "  #Note: \"\\(whitespace)\" is needed when we are calling shell command as a string via os.system\n",
        "RunTablePath = working_dir_path + RunTable_file\n",
        "cds_fasta_path = working_dir_path_ter + cds_fasta_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhFStAYYL5pv",
        "colab_type": "code",
        "outputId": "717a0cf8-ab86-4d20-acd3-c03674d9be56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Make new directory for this execution of the script if user chooses option \"A\"\n",
        "os.chdir(working_dir_path)\n",
        "\n",
        "if download_mode == \"A\":\n",
        "  date = str(dt.now().date())\n",
        "  files = os.listdir(working_dir_path)\n",
        "  try:\n",
        "      os.mkdir(working_dir_path + date + \"_01\")\n",
        "      print(date + \"_01 directory has been created.\") \n",
        "  except FileExistsError:\n",
        "      filename = max([filename for filename in files if date in filename])\n",
        "      file_serial_int = int(filename[-2:]) + 1\n",
        "      if 1 < file_serial_int < 10:\n",
        "          file_serial_str = \"0\" + str(file_serial_int)\n",
        "      elif 10 <= file_serial_int < 100:\n",
        "          file_serial_str = str(file_serial_int)\n",
        "          \n",
        "      os.mkdir(working_dir_path + date + \"_\" + file_serial_str)\n",
        "      print(date + \"_\" + file_serial_str + \" directory has been created.\")\n",
        "  except:\n",
        "      print(\"Directory failed to be created.\")\n",
        "\n",
        "#Calls the most recent directory\n",
        "#Will start from here for Option \"B\"\n",
        "files = os.listdir(working_dir_path)\n",
        "filename = max([filename for filename in files if date in filename])\n",
        "execution_dir_path = working_dir_path + filename + \"/\"\n",
        "execution_dir_path_ter = working_dir_path_ter + filename + \"/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-30_04 directory has been created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Dk6QbUrL9CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download report\n",
        "#Create a tab-separated .txt logfile that stores time and progress in this workflow\n",
        "os.chdir(execution_dir_path)\n",
        "#species_name = (RunTablePath.split('/')[-1]).split('_',1)[1][:-4]\n",
        "download_report_name = \"Download_report_\" + species_name + \"_\" + date + \".txt\"\n",
        "\n",
        "if os.path.exists(download_report_name):\n",
        "  pass\n",
        "  #For Option B, Download report will be read later in the for loop\n",
        "\n",
        "else:\n",
        "  #Create new download report\n",
        "  download_report = open(download_report_name, \"a+\")\n",
        "  download_report.write(\"Run ID\\tLibrary Layout\\tStatus\\tFile size\\tTimestamp\\tKallisto time (s)\\tn_processed\\tn_pseudoaligned\\tp_processed\\tp_pseudoaligned\\t%genes mapped\\n\")\n",
        "  download_report.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IplS1oKMB4A",
        "colab_type": "code",
        "outputId": "a71719cb-805b-457c-bf06-171477bc0d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#Create kallisto index\n",
        "kallisto_index_path_ter = execution_dir_path_ter + \"index_file_\" + species_name #to be created by kallisto\n",
        "kallisto_index_path = execution_dir_path + \"index_file_\" + species_name\n",
        "\n",
        "if os.path.exists(kallisto_index_path):\n",
        "  print(\"Kallisto index already present for \" + species_name + \".\")\n",
        "else:\n",
        "  index_start = time.time()\n",
        "  # os.system(kallisto_path + \" index -i \" + kallisto_index_path_ter + \" \" + cds_fasta_path)\n",
        "  !kallisto index -i $kallisto_index_path_ter $cds_fasta_path\n",
        "  if os.path.exists(kallisto_index_path):\n",
        "    print(\"Kallisto index created for \" + species_name + \".\")\n",
        "    print(\"Time to create kallisto index:\", time.time()-index_start)\n",
        "  else:\n",
        "    print(\"Kallisto index not found for \" + species_name + \".\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[build] loading fasta file /content/gdrive/My Drive/Projects/William_2019/Ntab/GCF_000715135.1_Ntab-TN90_cds_from_genomic.fna.gz\n",
            "[build] k-mer length: 31\n",
            "[build] warning: replaced 1436 non-ACGUT characters in the input sequence\n",
            "        with pseudorandom nucleotides\n",
            "[build] counting k-mers ... tcmalloc: large alloc 1610612736 bytes == 0x6b79c000 @  0x7ff7003151e7 0x6f181d 0x6f1899 0x4acad9 0x4a4ca8 0x4abe49 0x44e1d4 0x7ff6ff331b97 0x452a59\n",
            "done.\n",
            "[build] building target de Bruijn graph ...  done \n",
            "[build] creating equivalence classes ...  done\n",
            "[build] target de Bruijn graph has 1184086 contigs and contains 57392379 k-mers \n",
            "\n",
            "Kallisto index created for Ntab.\n",
            "Time to create kallisto index: 183.10516381263733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPYhT1VMS8_",
        "colab_type": "text"
      },
      "source": [
        "####Download functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ55RG81Etx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Functions#############################################################\n",
        "\n",
        "def get_ftp_links(RunID):\n",
        "  '''(str)->(str,str)\n",
        "  Return ftp link in the paired and unpaired format for the RunID specified\n",
        "  '''\n",
        "  dir2 = \"\"\n",
        "  if 9 < len(RunID) <= 12:\n",
        "      dir2 = \"0\"*(12 - len(RunID)) + RunID[-(len(RunID)-9):] + \"/\"\n",
        "  dirs = RunID[:6] + \"/\" + dir2 + RunID\n",
        "  ftp_link_paired = \"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/\" + dirs + \"/\" + RunID + \"_1.fastq.gz\"\n",
        "  ftp_link_unpaired = \"ftp://ftp.sra.ebi.ac.uk/vol1/fastq/\" + dirs + \"/\" + RunID + \".fastq.gz\"\n",
        "  return ftp_link_paired, ftp_link_unpaired\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def dlsize(RunID):\n",
        "  \"\"\"(str)->(str)\n",
        "  Returns size of downloaded file obtained from the log\n",
        "  \"\"\"\n",
        "  ll= open(RunID +\".log\", \"r\").readlines()[-1].split(\" \")\n",
        "  con = []\n",
        "  for i in ll:\n",
        "    if i != \" \":\n",
        "      con.append(i)\n",
        "    if len(con) >= 4:\n",
        "      break\n",
        "  return con[-1]\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def kallisto_stream(RunID):\n",
        "  '''(str)->(float,str,str,str,str)\n",
        "  Runs kallisto quant on streamed fastq file for each RunID, streaming the unpaired file first. If streaming for unpaired file fails, streaming will be attempted for paired file.\n",
        "  If both streaming for unpaired and paired files are unsuccessful, i.e. curl: (78) RETR response: 550; file probably does not exist on server and subsequent attempts will be aborted.\n",
        "  For streaming (curl command):\n",
        "    Streams only the first 1M bytes of data.\n",
        "    Ensures that:\n",
        "      Download speed not < 1x10^9 Bytes for 30 s\n",
        "      Maximum time taken = 600 s = 10 min\n",
        "    If these speed/time are not met, download is terminated and restarted for a total of 3 tries.\n",
        "  '''\n",
        "  RunID_file_path = execution_dir_path + RunID + \"/\"\n",
        "  paired, unpaired = get_ftp_links(RunID)\n",
        "  layout = \"Layout unknown\"\n",
        "  for i in range(3): #try downloading at most 3 times\n",
        "    kallisto_start = time.time()\n",
        "    timestamp = datetime.datetime.now()\n",
        "    # Dl first 1m bytes, max time 600s, speed limit ~1gb for 30s, send stderr to RunID.log\n",
        "    if layout == \"Paired\":\n",
        "      !kallisto quant -i $kallisto_index_path_ter -o $RunID --single -l 200 -s 20 -t 2 <(curl -L -r 0-1000000000 -m 600 --speed-limit 1000000 --speed-time 30 $paired 2> $RunID'.log')\n",
        "    else:\n",
        "      !kallisto quant -i $kallisto_index_path_ter -o $RunID --single -l 200 -s 20 -t 2 <(curl -L -r 0-1000000000 -m 600 --speed-limit 1000000 --speed-time 30 $unpaired 2> $RunID'.log')\n",
        "      if 'curl: (78)' not in open(RunID + '.log','r').read():\n",
        "       layout = \"Single\"\n",
        "      else:\n",
        "        !kallisto quant -i $kallisto_index_path_ter -o $RunID --single -l 200 -s 20 -t 2 <(curl -L -r 0-1000000000 -m 600 --speed-limit 1000000 --speed-time 30 $paired 2> $RunID'.log')\n",
        "        if 'curl: (78)' in open(RunID + '.log','r').read():\n",
        "          size = dlsize(RunID)\n",
        "          !rm $RunID\".log\"\n",
        "          print(RunID + '[i=' + str(i) + ']: File not found. Download aborted.')\n",
        "          layout = \"N/A\"\n",
        "          status = \"File not found\"\n",
        "          kallisto_end = time.time()\n",
        "          break\n",
        "        layout = \"Paired\"\n",
        "    kallisto_end = time.time()\n",
        "\n",
        "    #check for stderr logs, if absent, first 1GB file download is complete\n",
        "    size = dlsize(RunID)\n",
        "    if 'curl: (28)' not in open(RunID + '.log','r').read(): #if no slow dl speed error, acccept \n",
        "      !rm $RunID\".log\"\n",
        "      status = \"Downloaded\"\n",
        "      print(RunID + '[i=' + str(i) + ']: Download speed/time is acceptable.')\n",
        "      break\n",
        "    \n",
        "    if RunID + '.log' in os.listdir(execution_dir_path): #remove stderr log file before contining to the next attempt of download\n",
        "      !rm $RunID\".log\"\n",
        "      status = \"Download speed/time not accepted\"\n",
        "      print(RunID + '[i=' + str(i) + ']: Download speed/time is not accepted.')\n",
        "  \n",
        "  #If download still incomplete, use the last file saved\n",
        "  kallisto_time = round(kallisto_end - kallisto_start, 4)\n",
        "  \n",
        "  return kallisto_time, layout, status, size, timestamp\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def get_ListOfRunID(RunTablePath):\n",
        "  with open(RunTablePath,\"r\") as RunTable:\n",
        "    ListOfRunID = [RunID.strip().upper() for RunID in RunTable.readlines()]\n",
        "  if \"RUNID\" in ListOfRunID[0]: #If input file has header, exclude header\n",
        "    ListOfRunID = ListOfRunID[1:]\n",
        "  return ListOfRunID\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def open_download_report():\n",
        "  with open(download_report_name, \"r\") as download_report:\n",
        "    download_lines = download_report.readlines()\n",
        "  download_entries = [line.strip().split(\"\\t\") for line in download_lines]\n",
        "\n",
        "  return download_lines, download_entries\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def get_comments_index(download_lines):\n",
        "  hex_line_indices = [download_lines.index(line) for line in download_lines if \"#\" in line]\n",
        "  started_indices = [index for index in hex_line_indices if \"started\" in download_lines[index]]\n",
        "  completed_indices = [index for index in hex_line_indices if \"completed\" in download_lines[index]]\n",
        "\n",
        "  return hex_line_indices, started_indices, completed_indices\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def get_failed_RunID(mode_type):\n",
        "  '''\n",
        "  Opens Download Report;\n",
        "  Collate failed RunIDs from the latest COMPLETED j loop.\n",
        "  '''\n",
        "  if mode_type == \"A\": #CALLED WHEN MOVING ON TO THE NEXT J LOOP\n",
        "    started_index = -1\n",
        "  elif mode_type == \"B\": #CALLED WHEN REDOWNLOADING (j=1 or 2) WITH MODE B\n",
        "    started_index = -2\n",
        "  #Need to reopen download report to compile failed RunIDs\n",
        "  download_lines, download_entries = open_download_report()\n",
        "  hex_line_indices, started_indices, completed_indices = get_comments_index(download_lines)\n",
        "  \n",
        "  j_head_index = started_indices[started_index] # If 1<=j<=2, then 2<=len(started_indices)<=3\n",
        "\n",
        "  # Find failed RunIDs within last completed j loop, in chronological order\n",
        "  list_of_failed_RunID = []\n",
        "  for index in range(j_head_index, completed_indices[-1]):\n",
        "    if index not in hex_line_indices and download_entries[index][2] == \"Download speed/time not accepted\":\n",
        "      list_of_failed_RunID.append(download_entries[index][0])\n",
        "\n",
        "  return list_of_failed_RunID\n",
        "\n",
        "######################################################################\n",
        "\n",
        "\n",
        "def get_j():\n",
        "  '''\n",
        "  Get the current j loop download was paused at.\n",
        "  '''\n",
        "  download_lines, download_entries = open_download_report()\n",
        "  hex_line_indices, started_indices, completed_indices = get_comments_index(download_lines)\n",
        "  j = len(completed_indices)\n",
        "  return j\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def get_RunID_start(RunID_queue):\n",
        "  '''\n",
        "  * CALLED ONCE WHEN REDOWNLOADING WITH MODE B ONLY *\n",
        "\n",
        "  Checks from the bottom of Download Report upwards until the lastest #start.\n",
        "  Takes the latest RunID.\n",
        "  RunID_start_index will be the index of the next RunID in RunID_queue.\n",
        "  If all RunID in queue completed, index will simply = to len(RunID_queue),\n",
        "  will move on to next j loop\n",
        "  '''\n",
        "  download_lines, download_entries = open_download_report()\n",
        "  hex_line_indices, started_indices, completed_indices = get_comments_index(download_lines)\n",
        "\n",
        "  for i in range((len(download_entries) - 1), (started_indices[-1]), -1):\n",
        "    if i not in hex_line_indices:\n",
        "      RunID_latest = download_entries[i][0]\n",
        "      break\n",
        "\n",
        "  try:\n",
        "    RunID_start_index = RunID_queue.index(RunID_latest) + 1\n",
        "  except:\n",
        "    RunID_start_index = 0 # len(RunID_queue) will also be 0, move on to next j loop. \n",
        "    print(\"No RunID_start_index generated.\")\n",
        "\n",
        "  return RunID_start_index\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def update_download_report(to_print):\n",
        "  with open(download_report_name, \"a+\") as download_report:\n",
        "    download_report.write(to_print)\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def json_extract(RunID_file_path):\n",
        "  \"\"\"(str)->(str,str,str,str,str)\n",
        "  Extracts info from run_info.json\n",
        "  \"\"\"\n",
        "  kal_json = ast.literal_eval(open(RunID_file_path + \"run_info.json\", \"r\").read())\n",
        "  n_processed = str(kal_json[\"n_processed\"])\n",
        "  n_pseudoaligned = str(kal_json[\"n_pseudoaligned\"])\n",
        "  p_processed = str(kal_json[\"p_processed\"])\n",
        "  p_pseudoaligned = str(kal_json[\"p_pseudoaligned\"])\n",
        "  return n_processed, n_pseudoaligned, p_processed, p_pseudoaligned\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def abundance(RunID_file_path):\n",
        "  \"\"\"(str)->(str)\n",
        "  Computes percentage of genes mapped\n",
        "  \"\"\"\n",
        "  with open(RunID_file_path + \"abundance.tsv\", \"r\") as abun:\n",
        "    abun.readline()\n",
        "    tpmlist = [line.strip(\"\\n\").split(\"\\t\")[-1] for line in abun.readlines()]\n",
        "    tpmval = [val for val in tpmlist if val != \"0\" and val != \"-nan\"]\n",
        "    return str(round((len(tpmval)/len(tpmlist))*100 , 1)) + \"%\"\n",
        "\n",
        "######################################################################\n",
        "\n",
        "def download_loop(RunID_start_index, RunID_queue):\n",
        "  '''\n",
        "  Execute inner download_loop i=3 for all RunIDs in RunID_queue;\n",
        "  Updates download report as each RunID is processed.\n",
        "  '''\n",
        "  for index in range(RunID_start_index, len(RunID_queue)):\n",
        "    RunID = RunID_queue[index]\n",
        "    print()\n",
        "    print(\"-\"*40)\n",
        "    print()\n",
        "    job_queue = index + 1\n",
        "    total_queue = len(RunID_queue)\n",
        "    print('Processing ' + str(job_queue) + \"/\" + str(total_queue) + \": \" + RunID)\n",
        "\n",
        "    RunID_file_path = execution_dir_path + RunID + \"/\"\n",
        "    if os.path.exists(RunID_file_path) == False:\n",
        "      os.mkdir(RunID_file_path) #Directory to store kallisto files of each RunID\n",
        "    download_status = [RunID, \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"]\n",
        "    '''\n",
        "    *Download Report Headers*\n",
        "    Run ID | Library Layout | Status | File size | Timestamp | Kallisto time(s) | n_processed | n_pseudoaligned | p_processed | p_pseudoaligned | %genes mapped\n",
        "\n",
        "    *Possible output for \"Method\"*\n",
        "    download_status[2]\n",
        "      \"N/A\" -> No streaming attempted yet\n",
        "      \"Downloaded\" -> Successfully streamed and quantify single/paired-end data\n",
        "      \"File not found\" -> FASTQ file not found on server\n",
        "      \"Download speed/time not accepted\" -> Failed to stream at a satisfactory speed\n",
        "    '''\n",
        "    #Streaming method\n",
        "    kallisto_time, layout, status, size, timestamp = kallisto_stream(RunID)\n",
        "    if os.path.exists(RunID_file_path + \"run_info.json\"):\n",
        "      download_status[1] = layout\n",
        "      download_status[2] = status\n",
        "      download_status[3] = size\n",
        "      download_status[4] = str(kallisto_time)\n",
        "      download_status[5] = timestamp\n",
        "      n_processed, n_pseudoaligned, p_processed, p_pseudoaligned = json_extract(RunID_file_path)\n",
        "      download_status[6] = n_processed\n",
        "      download_status[7] = n_pseudoaligned\n",
        "      download_status[8] = p_processed\n",
        "      download_status[9] = p_pseudoaligned\n",
        "      download_status[10] = abundance(RunID_file_path)\n",
        "\n",
        "    else:\n",
        "      print(RunID + \": Missing kallisto output\")\n",
        "\n",
        "    update_download_report(\"\\t\".join(download_status) + \"\\n\")\n",
        "\n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flsp_U1UOefU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ListOfRunID = get_ListOfRunID(RunTablePath)\n",
        "\n",
        "# Specify variables for mode A or B\n",
        "\n",
        "if download_mode == \"A\":\n",
        "  j, RunID_start_index = 0, 0\n",
        "  RunID_queue = ListOfRunID\n",
        "\n",
        "elif download_mode == \"B\":\n",
        "  j = get_j()\n",
        "\n",
        "  if j == 3:\n",
        "    RunID_queue = [] #End download. j loop completed thrice.\n",
        "  else:\n",
        "    if j == 0:\n",
        "      RunID_queue = ListOfRunID\n",
        "    elif 1 <= j <= 2:\n",
        "      RunID_queue = get_failed_RunID(download_mode)\n",
        "    if RunID_queue == []: # End j loop if no more failed RunID.\n",
        "      j = 3\n",
        "  \n",
        "  RunID_start_index = get_RunID_start(RunID_queue)\n",
        "  update_download_report(\"#Download resumed\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N8rT19wtaNU",
        "colab_type": "text"
      },
      "source": [
        "####Download Loop\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT92KxG7EuIH",
        "colab_type": "code",
        "outputId": "121dfd3a-d9e1-44b0-9829-6985b1024307",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@title Code for Download Loop\n",
        "\n",
        "# Loop through j x i times down the RunID_queue\n",
        "\n",
        "for loop in range(j,3):\n",
        "  print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "  if RunID_start_index == 0:\n",
        "    print(\"Download attempt %s\"%(loop+1))\n",
        "    update_download_report(\"#Download attempt %s started\\n\" % (loop+1))\n",
        "  else:\n",
        "    print(\"Download attempt %s resumed\"%(j+1))\n",
        "  \n",
        "  download_loop(RunID_start_index, RunID_queue)\n",
        "  update_download_report(\"#Download attempt %s completed\\n\" % (loop+1))\n",
        "\n",
        "  #Reset RunID_start_index and RunID_queue\n",
        "  RunID_start_index = 0\n",
        "  RunID_queue = get_failed_RunID(\"A\")\n",
        "  if RunID_queue == []: # End j loop if no more failed RunID.\n",
        "      print(\"All RunIDs have been successfully downloaded.\")\n",
        "      break\n",
        "print(\"Download complete.\")\n",
        "with open(download_report_name, \"r\") as download_report:\n",
        "  file_content = download_report.read()\n",
        "  file_error = [(a.start(),a.end()) for a in list(re.finditer('File not found', file_content))]\n",
        "  file_success = [(a.start(),a.end()) for a in list(re.finditer('Downloaded', file_content))]\n",
        "  print(\"Total\\tDownloaded\\tRejected\\tFile not found\\n%s\\t%s\\t%s\\t%s\"\\\n",
        "        %(len(get_ListOfRunID(RunTablePath)),\\\n",
        "          len(file_success),\\\n",
        "          (len(get_ListOfRunID(RunTablePath))-len(file_success)-len(file_error)),\\\n",
        "          len(file_error)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------\n",
            "\n",
            "Download attempt 1\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Processing 1/7: SRR5381540\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x13c6000 @  0x7f241cae01e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f241bafcb97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 0 reads, 0 reads pseudoaligned\n",
            "[~warn] no reads pseudoaligned.\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 52 rounds\n",
            "\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x20aa000 @  0x7f0ca7dd41e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f0ca6df0b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 11,370,857 reads, 7,192,964 reads pseudoaligned\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 999 rounds\n",
            "\n",
            "SRR5381540[i=0]: Download speed/time is acceptable.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Processing 2/7: SRR1040767\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x1766000 @  0x7fab3f10a1e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7fab3e126b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 5,252,135 reads, 3,140,771 reads pseudoaligned\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 934 rounds\n",
            "\n",
            "SRR1040767[i=0]: Download speed/time is acceptable.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Processing 3/7: SRR3193977\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x212c000 @  0x7f020d1581e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f020c174b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 117,040 reads, 78,292 reads pseudoaligned\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 572 rounds\n",
            "\n",
            "SRR3193977[i=0]: Download speed/time is acceptable.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Processing 4/7: SRR1020542\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x1a38000 @  0x7f0d575241e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f0d56540b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 17,191,162 reads, 0 reads pseudoaligned\n",
            "[~warn] no reads pseudoaligned.\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 52 rounds\n",
            "\n",
            "SRR1020542[i=0]: Download speed/time is acceptable.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Processing 5/7: SRR1040769\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x1dac000 @  0x7f9b9dfc91e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f9b9cfe5b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 5,218,808 reads, 3,167,033 reads pseudoaligned\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 1,028 rounds\n",
            "\n",
            "SRR1040769[i=0]: Download speed/time is acceptable.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Processing 6/7: SRR5341129\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x1426000 @  0x7f97e0e3d1e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f97dfe59b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 0 reads, 0 reads pseudoaligned\n",
            "[~warn] no reads pseudoaligned.\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 52 rounds\n",
            "\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x2e76000 @  0x7f2f7952c1e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f2f78548b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 12,135,881 reads, 5,278,614 reads pseudoaligned\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 913 rounds\n",
            "\n",
            "SRR5341129[i=0]: Download speed/time is acceptable.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Processing 7/7: SRR5387726\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x1fc4000 @  0x7f0d659081e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f0d64924b97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 0 reads, 0 reads pseudoaligned\n",
            "[~warn] no reads pseudoaligned.\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 52 rounds\n",
            "\n",
            "\n",
            "[quant] fragment length distribution is truncated gaussian with mean = 200, sd = 20\n",
            "[index] k-mer length: 31\n",
            "[index] number of targets: 84,255\n",
            "[index] number of k-mers: 57,392,379\n",
            "tcmalloc: large alloc 1610612736 bytes == 0x264a000 @  0x7f2c266221e7 0x6f181d 0x6f1899 0x4acad9 0x4a6c50 0x44ec75 0x7f2c2563eb97 0x452a59\n",
            "[index] number of equivalence classes: 218,337\n",
            "[quant] running in single-end mode\n",
            "[quant] will process file 1: /dev/fd/63\n",
            "[quant] finding pseudoalignments for the reads ... done\n",
            "[quant] processed 11,969,281 reads, 7,811,348 reads pseudoaligned\n",
            "[   em] quantifying the abundances ... done\n",
            "[   em] the Expectation-Maximization algorithm ran for 945 rounds\n",
            "\n",
            "SRR5387726[i=0]: Download speed/time is acceptable.\n",
            "All RunIDs have been successfully downloaded.\n",
            "Download complete.\n",
            "Total\tDownloaded\tRejected\tFile not found\n",
            "7\t7\t0\t0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}